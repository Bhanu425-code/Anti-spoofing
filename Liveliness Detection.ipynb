{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from skimage import feature as skif\n",
    "import cv2\n",
    "import os\n",
    "import glob\n",
    "\n",
    "#\"C:\\Users\\bhanu\\Desktop\\gmm\\real_videos\\real\\20230130_160839_AdobeExpress.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lbp_histogram(image):\n",
    "    \"\"\"\n",
    "    Calculates the Local Binary Pattern (LBP) histogram for a given image.\n",
    "\n",
    "    Args:\n",
    "        image: The input image.\n",
    "\n",
    "    Returns:\n",
    "        hist_normalized: Normalized LBP histogram.\n",
    "    \"\"\"\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    lbp = skif.local_binary_pattern(gray_image, P=8, R=1, method='nri_uniform')\n",
    "    max_bins = int(lbp.max() + 1)\n",
    "    hist, _ = np.histogram(lbp.ravel(), bins=max_bins, range=(0, max_bins))\n",
    "    hist_normalized = hist / np.sum(hist)\n",
    "    return hist_normalized\n",
    "\n",
    "\n",
    "\n",
    "def get_lbp_hist(video_path):\n",
    "    \"\"\"\n",
    "    Calculates the average LBP feature vector for a given video.\n",
    "\n",
    "    Args:\n",
    "        video_path: The path to the input video.\n",
    "\n",
    "    Returns:\n",
    "        avg_feature_vector: Average LBP feature vector for the video.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    feature_vectors = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        feature_vector = lbp_histogram(frame)\n",
    "        feature_vectors.append(feature_vector)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    # Compute the average feature vector\n",
    "    avg_feature_vector = np.mean(feature_vectors, axis=0)\n",
    "\n",
    "    return avg_feature_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gmm_background_model(video_path):\n",
    "    \"\"\"\n",
    "    Creates a Gaussian Mixture Model (GMM) background model for a given video.\n",
    "\n",
    "    Args:\n",
    "        video_path: The path to the input video.\n",
    "\n",
    "    Returns:\n",
    "        background_model: The generated background model image.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    bg_sub = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        fg_mask = bg_sub.apply(frame)\n",
    "\n",
    "    background_model = bg_sub.getBackgroundImage()\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    return background_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_2d_fft(frame):\n",
    "    \"\"\"\n",
    "    Applies a 2D Fast Fourier Transform (FFT) to a given frame.\n",
    "\n",
    "    Args:\n",
    "        frame: The input frame.\n",
    "\n",
    "    Returns:\n",
    "        magnitude_spectrum: Magnitude spectrum obtained from the FFT.\n",
    "    \"\"\"\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    fft = np.fft.fft2(gray)\n",
    "    fft_shifted = np.fft.fftshift(fft)\n",
    "    magnitude_spectrum = np.abs(fft_shifted)\n",
    "\n",
    "    return magnitude_spectrum\n",
    "\n",
    "def fft_feature(video_path):\n",
    "    \"\"\"\n",
    "    Computes the average frequency feature vector using FFT for a given video.\n",
    "\n",
    "    Args:\n",
    "        video_path: The path to the input video.\n",
    "\n",
    "    Returns:\n",
    "        feature_3: Average frequency feature vector.\n",
    "    \"\"\"\n",
    "    background_model = create_gmm_background_model(video_path)\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    frames = []\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        foreground = cv2.absdiff(frame, background_model)\n",
    "        frames.append(foreground)\n",
    "\n",
    "    cap.release()\n",
    "    feature = []\n",
    "    print(len(frames))\n",
    "    for frame in frames:\n",
    "        frequency_feature = apply_2d_fft(frame)\n",
    "        feature.append(frequency_feature)\n",
    "\n",
    "    feature_3 = np.mean(feature, axis=(0, 1, 2))\n",
    "\n",
    "    return feature_3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gmmfeatures(video_path):\n",
    "    \"\"\"\n",
    "    Computes GMM features for a given video.\n",
    "\n",
    "    Args:\n",
    "        video_path: The path to the input video.\n",
    "\n",
    "    Returns:\n",
    "        A list containing the calculated GMM features: [cmd, avgmEntropy].\n",
    "    \"\"\"\n",
    "    fgP = []   # List to store foreground pixel counts for each frame\n",
    "    bgP = []   # List to store background pixel counts for each frame\n",
    "    bgA = []   # List to store background area counts for each frame\n",
    "    fgA = []   # List to store foreground area counts for each frame\n",
    "\n",
    "    # Open the video using cv2.VideoCapture and obtain the total number of frames in the video\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Create a background subtractor using cv2.createBackgroundSubtractorMOG2()\n",
    "    bg_subtractor = cv2.createBackgroundSubtractorMOG2()\n",
    "\n",
    "    # Loop through each frame in the video\n",
    "    while cap.isOpened:\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If the frame is not successfully read (end of video), exit the loop\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Apply background subtraction to the frame, creating a binary mask where foreground is white and background is black\n",
    "        fg_mask = bg_subtractor.apply(frame)\n",
    "\n",
    "        # Count the number of non-zero (white) pixels in the foreground mask, representing the number of foreground pixels\n",
    "        fg_pixels = cv2.countNonZero(fg_mask)\n",
    "\n",
    "        # Calculate the number of background pixels by subtracting the number of foreground pixels from the total number of pixels in the mask\n",
    "        bg_pixels = fg_mask.size - fg_pixels\n",
    "\n",
    "        # Append the number of foreground pixels to the list fgP\n",
    "        fgP.append(fg_pixels)\n",
    "\n",
    "        # Append the number of background pixels to the list bgP\n",
    "        bgP.append(bg_pixels)\n",
    "\n",
    "    # Release the video capture\n",
    "    cap.release()\n",
    "\n",
    "    # Close any remaining windows\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Open the video using cv2.VideoCapture\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "    # Load the Haar cascade classifier for face detection\n",
    "    face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "    # Get the width and height of the video frames\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "\n",
    "    # Calculate the total area of the video frame\n",
    "    total_area = width * height\n",
    "\n",
    "    # Loop through each frame in the video\n",
    "    while cap.isOpened:\n",
    "        # Read the next frame\n",
    "        ret, frame = cap.read()\n",
    "\n",
    "        # If the frame is not successfully read (end of video), exit the loop\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Convert the frame to grayscale\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "        # Detect faces in the grayscale frame using the Haar cascade classifier\n",
    "        faces = face_cascade.detectMultiScale(gray, scaleFactor=1.3, minNeighbors=5)\n",
    "\n",
    "        # Initialize variables for face area and background area\n",
    "        face_area = 0\n",
    "        bg_area = total_area\n",
    "\n",
    "        # Calculate the total face area and background area in the frame\n",
    "        for (x, y, w, h) in faces:\n",
    "            face_area += w * h\n",
    "            bg_area -= w * h\n",
    "\n",
    "        # Draw rectangles around the detected faces in the frame\n",
    "        for (x, y, w, h) in faces:\n",
    "            cv2.rectangle(frame, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "\n",
    "        # Append the face area and background area to their respective lists\n",
    "        fgA.append(face_area)\n",
    "        bgA.append(bg_area)\n",
    "\n",
    "    # Release the video capture\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "    mt1 = []  # List to store motion term 1 values\n",
    "    mt2 = []  # List to store motion term 2 values\n",
    "    for i in range(len(fgA)):\n",
    "        epsilon = 1e-6\n",
    "        if fgA[i] != 0:\n",
    "            mtf = fgP[i] / fgA[i]\n",
    "        else:\n",
    "            mtf = epsilon\n",
    "        mt1.append(mtf)\n",
    "    for i in range(len(bgA)):\n",
    "        mtb = fgP[i] / bgA[i]\n",
    "        mt2.append(mtb)\n",
    "\n",
    "    nm = 0\n",
    "    cmd = 0\n",
    "    dm = 0\n",
    "    motion_features = []\n",
    "    mEntropyList = []  # List to store motion entropy values\n",
    "    for i in range(len(mt1)):\n",
    "        epsilon = 1e-6\n",
    "        nm = (mt1[i] - mt2[i])**2\n",
    "        dm = (mt1[i] + mt2[i])\n",
    "        if dm != 0:\n",
    "            cmd += (nm / dm)\n",
    "        else:\n",
    "            cmd += epsilon\n",
    "        pi = fgP[i] / (fgP[i] + bgP[i])\n",
    "        u = 1 / (width * height)\n",
    "        if pi >= 1:\n",
    "            pi = min(pi, 1 - u)\n",
    "        elif pi == 0:\n",
    "            pi = max(pi, u)\n",
    "\n",
    "\n",
    "        me=-pi*math.log(pi)-(1-pi)*math.log(1-pi)\n",
    "        mEntropyList.append(me)\n",
    "    cmd=cmd/total_frames\n",
    "    print(total_frames)\n",
    "    mEntropysum=0\n",
    "    avgmEntropy=0\n",
    "    for i in range(len(mEntropyList)):\n",
    "        mEntropysum+=mEntropyList[i]\n",
    "    avgmEntropy=mEntropysum/len(mEntropyList)\n",
    "    return [cmd,avgmEntropy]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate(video_path):\n",
    "    \"\"\"\n",
    "    Concatenates multiple feature vectors into a single vector for a given video.\n",
    "\n",
    "    Args:\n",
    "        video_path: The path to the input video.\n",
    "\n",
    "    Returns:\n",
    "        concatenated_vector: The concatenated feature vector.\n",
    "    \"\"\"\n",
    "    feature_vector_1 = get_lbp_hist(video_path)\n",
    "    feature_vector_1 = np.array(feature_vector_1)\n",
    "\n",
    "    feature_vector_2 = gmmfeatures(video_path)\n",
    "    feature_vector_2 = np.array(feature_vector_2)\n",
    "\n",
    "    feature_vector_3 = fft_feature(video_path)\n",
    "    feature_vector_3 = np.array(feature_vector_3)\n",
    "    feature_vector_3 = fft_feature(video_path)\n",
    "\n",
    "\n",
         
         feature_vector_3 = np.reshape(feature_vector_3, (1,)) 
    "    concatenated_vector = np.concatenate((feature_vector_1, feature_vector_2, feature_vector_3))\n",
    "\n",
    "    return concatenated_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"C:\\Users\\bhanu\\Desktop\\gmm\\real\"  # Replace with the actual path to your folder\n",
    "real_video_paths = []  # List to store the paths of the real video files\n",
    "video_extensions = ['.mp4', '.mov']  # Add more extensions if needed\n",
    "\n",
    "# Iterate through each video extension\n",
    "for extension in video_extensions:\n",
    "    search_pattern = os.path.join(folder_path, f'*{extension}')\n",
    "    # Use glob.glob to search for files matching the search pattern\n",
    "    # and extend the real_video_paths list with the matched file paths\n",
    "    real_video_paths.extend(glob.glob(search_pattern))\n",
    "\n",
    "print(len(real_video_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "126\n"
     ]
    }
   ],
   "source": [
    "folder_path = r\"C:\\Users\\bhanu\\Desktop\\gmm\\test_sample\"  # Replace with the actual path to your folder\n",
    "fake_video_paths = []  # List to store the paths of the fake video files\n",
    "video_extensions = ['.mp4', '.mov']  # Add more extensions if needed\n",
    "\n",
    "# Iterate through each video extension\n",
    "for extension in video_extensions:\n",
    "    search_pattern = os.path.join(folder_path, f'*{extension}')\n",
    "    # Use glob.glob to search for files matching the search pattern\n",
    "    # and extend the fake_video_paths list with the matched file paths\n",
    "    fake_video_paths.extend(glob.glob(search_pattern))\n",
    "\n",
    "print(len(fake_video_paths))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_access = []  # List to store the feature vectors for real videos\n",
    "\n",
    "# Iterate through each real video path\n",
    "for video_path in real_video_paths:\n",
    "    # Compute the concatenated feature vector for the current video\n",
    "    concatenated_features = concatenate(video_path)\n",
    "    # Append the feature vector to the real_access list\n",
    "    real_access.append(concatenated_features)\n",
    "\n",
    "print(len(real_access))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fake_access = []  # List to store the feature vectors for fake videos\n",
    "\n",
    "# Iterate through each fake video path\n",
    "for video_path in fake_video_paths:\n",
    "    # Compute the concatenated feature vector for the current video\n",
    "    concatenated_features = concatenate(video_path)\n",
    "    # Append the feature vector to the fake_access list\n",
    "    fake_access.append(concatenated_features)\n",
    "\n",
    "print(len(fake_access))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_feature_vector = []  # List to store all feature vectors\n",
    "\n",
    "# Combine the feature vectors from real and fake videos into a single list\n",
    "all_feature_vector = real_access + fake_access\n",
    "\n",
    "print(len(all_feature_vector))\n",
    "\n",
    "# Create labels for the corresponding videos\n",
    "labels = np.array([0] * len(real_access) + [1] * len(fake_access))\n",
    "\n",
    "print(len(labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in all_feature_vector:\n",
    "    print(len(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from hmmlearn import hmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_feature_vector, labels, test_size=0.4, random_state=4)\n",
    "\n",
    "# Initializing a Gaussian Hidden Markov Model (HMM)\n",
    "hmm_model = hmm.GaussianHMM()\n",
    "\n",
    "# Fitting the HMM model to the training data\n",
    "hmm_model.fit(X_train) \n",
    "\n",
    "# Generating HMM-based features for the training data\n",
    "hmm_features_train = hmm_model.predict_proba(X_train)\n",
    "\n",
    "# Generating HMM-based features for the testing data\n",
    "hmm_features_test = hmm_model.predict_proba(X_test)\n",
    "\n",
    "# Combining the original features with the HMM-based features for the training data\n",
    "combined_features_train = np.concatenate((X_train, hmm_features_train), axis=1)\n",
    "\n",
    "# Combining the original features with the HMM-based features for the testing data\n",
    "combined_features_test = np.concatenate((X_test, hmm_features_test), axis=1)\n",
    "\n",
    "# Initializing a Support Vector Machine (SVM) classifier\n",
    "svm_classifier = svm.SVC()  \n",
    "\n",
    "# Fitting the SVM classifier to the combined training features and corresponding labels\n",
    "svm_classifier.fit(combined_features_train, y_train)\n",
    "\n",
    "# Predicting labels for the testing data using the trained SVM classifier\n",
    "y_pred = svm_classifier.predict(combined_features_test)\n",
    "\n",
    "# Calculating the accuracy of the predicted labels compared to the true labels\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "\n",
    "# Calculating the precision of the predicted labels compared to the true labels\n",
    "precision = precision_score(y_test, y_pred)\n",
    "\n",
    "# Calculating the recall of the predicted labels compared to the true labels\n",
    "recall = recall_score(y_test, y_pred)\n",
    "\n",
    "# Calculating the F1 score of the predicted labels compared to the true labels\n",
    "f1 = f1_score(y_test, y_pred)\n",
    "\n",
    "# Printing the accuracy, precision, recall, and F1 score\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Precision:\", precision)\n",
    "print(\"Recall:\", recall)\n",
    "print(\"F1 score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "# Calculate the confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Print the confusion matrix\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "import matplotlib.pyplot as plt\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "auc_roc = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "# Plotting the ROC curve\n",
    "plt.plot(fpr, tpr, label='ROC curve (AUC = %0.2f)' % auc_roc)\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "joblib.dump(svm_classifier, 'model.pkl')\n",
    "\n",
    "model = joblib.load(r\"C:\\Users\\bhanu\\Desktop\\gmm\\model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_access=[]\n",
    "video_path=r\"C:\\Users\\bhanu\\Desktop\\gmm\\attack\\attack_client034_android_SD_ipad_video_scene01.mp4\"  #new fake video from mfsu mfsd data\n",
    "\n",
    "concatenated_features=concatenate(video_path)\n",
    "new_access.append(concatenated_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Convert new_access list to numpy array\n",
    "new_access = np.array(new_access)\n",
    "\n",
    "additional_feature = np.zeros((new_access.shape[0], 1))\n",
    "\n",
    "new_access_with_additional_feature = np.concatenate((new_access, additional_feature), axis=1)\n",
    "\n",
    "\n",
    "\n",
    "# Perform prediction\n",
    "predicted_class = model.predict(new_access_with_additional_feature)\n",
    "print(predicted_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\bhanu\\Desktop\\gmm\\fake_videos\\fake\" # Replace with the actual path to your folder\n",
    "new_fake_video_paths=[]\n",
    "video_extensions = ['.mp4', '.mov']  # Add more extensions if needed\n",
    "for extension in video_extensions:\n",
    "    search_pattern = os.path.join(folder_path, f'*{extension}')\n",
    "    new_fake_video_paths.extend(glob.glob(search_pattern))\n",
    "print(len(new_fake_video_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_path = r\"C:\\Users\\bhanu\\Desktop\\gmm\\real_videos\\real\" # Replace with the actual path to your folder\n",
    "new_real_video_paths=[]\n",
    "video_extensions = ['.mp4', '.mov']  # Add more extensions if needed\n",
    "for extension in video_extensions:\n",
    "    search_pattern = os.path.join(folder_path, f'*{extension}')\n",
    "    new_real_video_paths.extend(glob.glob(search_pattern))\n",
    "print(len(new_real_video_paths))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#real videos feature vectors\n",
    "new_real_access=[]\n",
    "for video_path in new_real_video_paths:\n",
    "    concatenated_features=concatenate(video_path)\n",
    "    new_real_access.append(concatenated_features)\n",
    "print(len(new_real_access))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_fake_access=[]\n",
    "for video_path in new_fake_video_paths:\n",
    "    concatenated_features=concatenate(video_path)\n",
    "    new_fake_access.append(concatenated_features)\n",
    "print(len(new_fake_access))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_new_feature_vector=[]\n",
    "all_new_feature_vector=new_real_access+new_fake_access\n",
    "print(len(all_new_feature_vector))\n",
    "new_labels = np.array([0] * len(new_real_access) + [1] * len(new_fake_access))\n",
    "print(len(new_labels))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hmm_model = hmm.GaussianHMM()\n",
    "\n",
    "hmm_model.fit(X_train) \n",
    "hmm_features_train = hmm_model.predict_proba(all_feature_vector)\n",
    "hmm_features_test = hmm_model.predict_proba(all_new_feature_vector)\n",
    "svm_classifier = svm.SVC()  \n",
    "svm_classifier.fit(hmm_features_train, labels)\n",
    "y_pred = svm_classifier.predict(hmm_features_test)\n",
    "\n",
    "accuracy = accuracy_score(new_labels, y_pred)\n",
    "precision = precision_score(new_labels, y_pred)\n",
    "recall = recall_score(new_labels, y_pred)\n",
    "f1 = f1_score(new_labels, y_pred)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"precision:\",precision)\n",
    "print(\"recall\",recall)\n",
    "print(\"f1:\",f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
